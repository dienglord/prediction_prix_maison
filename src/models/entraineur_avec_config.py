import pandas as pd
import numpy as np
import logging
from pathlib import Path
import sys
import joblib
from typing import Dict, Any, Tuple, List
import warnings
from datetime import datetime
import os
warnings.filterwarnings('ignore')

# MLflow imports
import mlflow
import mlflow.sklearn
from mlflow.models.signature import infer_signature

# Mod√®les ML
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns

# Configuration centralis√©e
sys.path.append(str(Path(__file__).resolve().parents[2]))
from src.config.gestionnaire_config import get_config_manager

class EntraineurAvecConfiguration:
    """
    Entra√Æneur de mod√®les utilisant la configuration centralis√©e.
    Version MLOps Phase 2 - Configuration Management.
    """
    
    def __init__(self, environment: str = None):
        # D√©terminer l'environnement
        self.environment = environment or os.getenv('MLOPS_ENV', 'development')
        
        # Charger la configuration
        self.config_manager = get_config_manager(self.environment)
        self.config = self.config_manager.config
        
        if not self.config:
            raise ValueError(f"Impossible de charger la configuration pour: {self.environment}")
        
        # Configuration du projet
        self.racine_projet = self.config_manager.project_root
        
        # Configuration du logging
        self.setup_logging()
        
        # Configuration MLflow
        self.setup_mlflow()
        
        # Cr√©er les mod√®les depuis la configuration
        self.modeles_config = self.create_models_from_config()
        
        # Stockage des r√©sultats
        self.resultats_entrainement = {}
        self.meilleur_modele = None
        self.meilleur_score = float('-inf')
        self.modele_final = None
        self.meilleur_run_id = None
        
        self.logger.info(f"üî• Entra√Æneur configur√© - Environnement: {self.environment}")
        self.logger.info(f"üìä Mod√®les activ√©s: {list(self.modeles_config.keys())}")
        self.logger.info(f"üêõ Mode debug: {self.config.debug}")
    
    def setup_logging(self):
        """Configure le syst√®me de logging depuis la configuration."""
        logs_dir = self.racine_projet / self.config.logging.file_path
        logs_dir.mkdir(exist_ok=True)
        
        # Configuration du niveau de logging
        level = getattr(logging, self.config.logging.level.upper())
        
        handlers = []
        
        # Handler fichier si activ√©
        if self.config.logging.file_enabled:
            file_handler = logging.FileHandler(
                logs_dir / f"training_{self.environment}.log", 
                encoding='utf-8'
            )
            file_handler.setFormatter(logging.Formatter(self.config.logging.format))
            handlers.append(file_handler)
        
        # Handler console si activ√©
        if self.config.logging.console_enabled:
            console_handler = logging.StreamHandler(sys.stdout)
            console_handler.setFormatter(logging.Formatter(self.config.logging.format))
            handlers.append(console_handler)
        
        logging.basicConfig(
            level=level,
            format=self.config.logging.format,
            handlers=handlers,
            force=True  # Override existing config
        )
        
        self.logger = logging.getLogger(f"entraineur_{self.environment}")
    
    def setup_mlflow(self):
        """Configure MLflow depuis la configuration."""
        try:
            # Configuration MLflow
            mlflow.set_tracking_uri(self.config.mlflow.tracking_uri)
            
            # Configuration de l'exp√©rience
            try:
                experiments = mlflow.search_experiments()
                experiment_found = False
                
                for exp in experiments:
                    if exp.name == self.config.mlflow.experiment_name:
                        mlflow.set_experiment(exp.experiment_id)
                        self.logger.info(f"‚úÖ Exp√©rience MLflow: {exp.name} (ID: {exp.experiment_id})")
                        experiment_found = True
                        break
                
                if not experiment_found:
                    # Cr√©er l'exp√©rience si elle n'existe pas
                    exp_id = mlflow.create_experiment(
                        name=self.config.mlflow.experiment_name,
                        artifact_location=self.config.mlflow.artifact_location
                    )
                    mlflow.set_experiment(exp_id)
                    self.logger.info(f"‚úÖ Nouvelle exp√©rience MLflow: {self.config.mlflow.experiment_name}")
                
            except Exception as exp_error:
                self.logger.warning(f"‚ö†Ô∏è Probl√®me exp√©rience MLflow: {str(exp_error)}")
                # Utiliser l'exp√©rience par d√©faut
                experiments = mlflow.search_experiments()
                if experiments:
                    mlflow.set_experiment(experiments[0].experiment_id)
                    self.logger.info("üîÑ Utilisation exp√©rience par d√©faut")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur configuration MLflow: {str(e)}")
    
    def create_models_from_config(self) -> Dict[str, Any]:
        """Cr√©e les mod√®les depuis la configuration."""
        modeles = {}
        
        # R√©cup√©rer seulement les mod√®les activ√©s
        enabled_models = self.config_manager.get_enabled_models()
        
        for nom_modele, model_config in enabled_models.items():
            try:
                # Cr√©er le mod√®le selon l'algorithme
                if model_config.algorithm == "LinearRegression":
                    modele = LinearRegression(**model_config.hyperparameters)
                    
                elif model_config.algorithm == "RandomForestRegressor":
                    modele = RandomForestRegressor(**model_config.hyperparameters)
                    
                elif model_config.algorithm == "GradientBoostingRegressor":
                    modele = GradientBoostingRegressor(**model_config.hyperparameters)
                    
                else:
                    self.logger.warning(f"‚ö†Ô∏è Algorithme non support√©: {model_config.algorithm}")
                    continue
                
                modeles[nom_modele] = {
                    'model': modele,
                    'config': model_config
                }
                
                self.logger.info(f"‚úÖ Mod√®le cr√©√©: {nom_modele} ({model_config.algorithm})")
                
            except Exception as e:
                self.logger.error(f"‚ùå Erreur cr√©ation mod√®le {nom_modele}: {str(e)}")
                continue
        
        return modeles
    
    def charger_donnees_preprocessees(self) -> Tuple[pd.DataFrame, ...]:
        """Charge les donn√©es pr√©process√©es depuis la configuration."""
        self.logger.info("üìä Chargement des donn√©es pr√©process√©es...")
        
        processed_dir = self.racine_projet / self.config.data.processed_path
        
        try:
            X_train = pd.read_csv(processed_dir / "X_train.csv")
            X_val = pd.read_csv(processed_dir / "X_val.csv")
            X_test = pd.read_csv(processed_dir / "X_test.csv")
            y_train = pd.read_csv(processed_dir / "y_train.csv").iloc[:, 0]
            y_val = pd.read_csv(processed_dir / "y_val.csv").iloc[:, 0]
            y_test = pd.read_csv(processed_dir / "y_test.csv").iloc[:, 0]
            
            # Validation des features configur√©es
            expected_features = self.config.data.features
            actual_features = list(X_train.columns)
            
            if set(expected_features) != set(actual_features):
                self.logger.warning(f"‚ö†Ô∏è Features diff√©rentes de la configuration:")
                self.logger.warning(f"   Configur√©es: {expected_features}")
                self.logger.warning(f"   Trouv√©es: {actual_features}")
            
            self.logger.info(f"‚úÖ Donn√©es charg√©es:")
            self.logger.info(f"   - Train: {X_train.shape}")
            self.logger.info(f"   - Validation: {X_val.shape}")
            self.logger.info(f"   - Test: {X_test.shape}")
            self.logger.info(f"   - Features: {len(actual_features)}")
            
            return X_train, X_val, X_test, y_train, y_val, y_test
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors du chargement: {str(e)}")
            raise
    
    def calculer_metriques(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """Calcule les m√©triques de performance."""
        metriques = {
            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
            'mae': mean_absolute_error(y_true, y_pred),
            'r2': r2_score(y_true, y_pred),
            'mape': np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
        }
        
        return metriques
    
    def entrainer_modele_avec_mlflow(self, nom_modele: str, model_info: Dict[str, Any],
                                   X_train: pd.DataFrame, y_train: pd.Series,
                                   X_val: pd.DataFrame, y_val: pd.Series) -> Dict[str, Any]:
        """Entra√Æne un mod√®le avec tracking MLflow utilisant la configuration."""
        
        modele = model_info['model']
        model_config = model_info['config']
        
        # Nom de run bas√© sur la configuration
        run_name = f"{nom_modele}_{self.environment}_{datetime.now().strftime('%H%M%S')}"
        
        try:
            with mlflow.start_run(run_name=run_name) as run:
                self.logger.info(f"üî• MLflow Run: {run.info.run_id}")
                self.logger.info(f"üîß Entra√Ænement: {nom_modele}")
                
                # === LOGGING DES PARAM√àTRES DE CONFIGURATION ===
                mlflow.log_param("environment", self.environment)
                mlflow.log_param("model_name", nom_modele)
                mlflow.log_param("algorithm", model_config.algorithm)
                mlflow.log_param("model_description", model_config.description)
                
                # Hyperparam√®tres depuis la configuration
                mlflow.log_params(model_config.hyperparameters)
                
                # Param√®tres de donn√©es depuis la configuration
                mlflow.log_param("test_size", self.config.data.test_size)
                mlflow.log_param("val_size", self.config.data.val_size)
                mlflow.log_param("random_state", self.config.data.random_state)
                mlflow.log_param("n_features", len(self.config.data.features))
                
                # === ENTRA√éNEMENT ===
                start_time = datetime.now()
                modele.fit(X_train, y_train)
                training_time = (datetime.now() - start_time).total_seconds()
                
                mlflow.log_metric("training_time_seconds", training_time)
                
                # === PR√âDICTIONS ===
                y_pred_train = modele.predict(X_train)
                y_pred_val = modele.predict(X_val)
                
                # === M√âTRIQUES ===
                metriques_train = self.calculer_metriques(y_train, y_pred_train)
                metriques_val = self.calculer_metriques(y_val, y_pred_val)
                
                # Validation crois√©e
                cv_scores = cross_val_score(modele, X_train, y_train, cv=5, 
                                          scoring='neg_mean_squared_error')
                cv_rmse = np.sqrt(-cv_scores)
                
                # √âcart train/validation
                ecart_r2 = metriques_train['r2'] - metriques_val['r2']
                
                # === LOGGING DES M√âTRIQUES ===
                mlflow.log_metric("train_rmse", metriques_train['rmse'])
                mlflow.log_metric("train_r2", metriques_train['r2'])
                mlflow.log_metric("train_mae", metriques_train['mae'])
                
                mlflow.log_metric("val_rmse", metriques_val['rmse'])
                mlflow.log_metric("val_r2", metriques_val['r2'])
                mlflow.log_metric("val_mae", metriques_val['mae'])
                
                mlflow.log_metric("cv_rmse_mean", cv_rmse.mean())
                mlflow.log_metric("cv_rmse_std", cv_rmse.std())
                mlflow.log_metric("overfitting_gap", ecart_r2)
                
                # === VALIDATION DES CRIT√àRES DE PERFORMANCE ===
                # V√©rifier les crit√®res d√©finis dans la configuration des mod√®les
                performance_ok = True
                if hasattr(model_config, 'performance_criteria'):
                    criteria = getattr(model_config, 'performance_criteria', {})
                    
                    min_r2 = criteria.get('min_r2_score', 0)
                    max_rmse = criteria.get('max_rmse', float('inf'))
                    max_time = criteria.get('max_training_time_seconds', float('inf'))
                    
                    if metriques_val['r2'] < min_r2:
                        performance_ok = False
                        mlflow.set_tag("performance_warning", f"R2 ({metriques_val['r2']:.4f}) < min ({min_r2})")
                    
                    if metriques_val['rmse'] > max_rmse:
                        performance_ok = False
                        mlflow.set_tag("performance_warning", f"RMSE ({metriques_val['rmse']:.0f}) > max ({max_rmse})")
                    
                    if training_time > max_time:
                        performance_ok = False
                        mlflow.set_tag("performance_warning", f"Temps ({training_time:.1f}s) > max ({max_time}s)")
                
                # === LOGGING DU MOD√àLE ===
                if self.config.mlflow.log_models:
                    mlflow.sklearn.log_model(
                        sk_model=modele,
                        artifact_path="model"
                    )
                
                # === TAGS ===
                mlflow.set_tag("environment", self.environment)
                mlflow.set_tag("model_family", "regression")
                mlflow.set_tag("use_case", "prix_immobilier")
                mlflow.set_tag("performance_ok", str(performance_ok))
                
                if self.config.debug:
                    mlflow.set_tag("debug_mode", "true")
                
                # === LOGGING CONSOLE ===
                self.logger.info(f"‚úÖ {nom_modele} entra√Æn√©:")
                self.logger.info(f"   - R¬≤ validation: {metriques_val['r2']:.4f}")
                self.logger.info(f"   - RMSE validation: {metriques_val['rmse']:.2f}")
                self.logger.info(f"   - Temps d'entra√Ænement: {training_time:.2f}s")
                self.logger.info(f"   - Performance OK: {performance_ok}")
                self.logger.info(f"   - MLflow Run ID: {run.info.run_id}")
                
                # Pr√©parer les r√©sultats
                resultats = {
                    'modele': modele,
                    'nom': nom_modele,
                    'run_id': run.info.run_id,
                    'metriques_train': metriques_train,
                    'metriques_val': metriques_val,
                    'cv_rmse_mean': cv_rmse.mean(),
                    'ecart_r2': ecart_r2,
                    'training_time': training_time,
                    'performance_ok': performance_ok,
                    'y_pred_train': y_pred_train,
                    'y_pred_val': y_pred_val
                }
                
                return resultats
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è MLflow indisponible pour {nom_modele}: {str(e)}")
            self.logger.info(f"üîÑ Entra√Ænement sans MLflow...")
            
            # Fallback sans MLflow
            start_time = datetime.now()
            modele.fit(X_train, y_train)
            training_time = (datetime.now() - start_time).total_seconds()
            
            y_pred_train = modele.predict(X_train)
            y_pred_val = modele.predict(X_val)
            
            metriques_train = self.calculer_metriques(y_train, y_pred_train)
            metriques_val = self.calculer_metriques(y_val, y_pred_val)
            
            cv_scores = cross_val_score(modele, X_train, y_train, cv=5, 
                                      scoring='neg_mean_squared_error')
            cv_rmse = np.sqrt(-cv_scores)
            ecart_r2 = metriques_train['r2'] - metriques_val['r2']
            
            self.logger.info(f"‚úÖ {nom_modele} (sans MLflow):")
            self.logger.info(f"   - R¬≤ validation: {metriques_val['r2']:.4f}")
            self.logger.info(f"   - RMSE validation: {metriques_val['rmse']:.2f}")
            self.logger.info(f"   - Temps d'entra√Ænement: {training_time:.2f}s")
            
            return {
                'modele': modele,
                'nom': nom_modele,
                'run_id': 'no_mlflow',
                'metriques_train': metriques_train,
                'metriques_val': metriques_val,
                'cv_rmse_mean': cv_rmse.mean(),
                'ecart_r2': ecart_r2,
                'training_time': training_time,
                'performance_ok': True,  # Assume OK in fallback
                'y_pred_train': y_pred_train,
                'y_pred_val': y_pred_val
            }
    
    def entrainer_tous_modeles(self, X_train: pd.DataFrame, y_train: pd.Series,
                              X_val: pd.DataFrame, y_val: pd.Series) -> Dict[str, Any]:
        """Entra√Æne tous les mod√®les configur√©s."""
        self.logger.info("üî• === ENTRA√éNEMENT AVEC CONFIGURATION ===")
        self.logger.info(f"üìä Environnement: {self.environment}")
        self.logger.info(f"ü§ñ Mod√®les √† entra√Æner: {list(self.modeles_config.keys())}")
        
        self.resultats_entrainement = {}
        
        for nom_modele, model_info in self.modeles_config.items():
            try:
                resultats = self.entrainer_modele_avec_mlflow(
                    nom_modele, model_info, X_train, y_train, X_val, y_val
                )
                self.resultats_entrainement[nom_modele] = resultats
                
                # Suivre le meilleur mod√®le
                r2_val = resultats['metriques_val']['r2']
                if r2_val > self.meilleur_score:
                    self.meilleur_score = r2_val
                    self.meilleur_modele = nom_modele
                    self.modele_final = resultats['modele']
                    self.meilleur_run_id = resultats['run_id']
                
            except Exception as e:
                self.logger.error(f"‚ùå √âchec {nom_modele}: {str(e)}")
                continue
        
        self.logger.info(f"üèÜ Meilleur mod√®le: {self.meilleur_modele} (R¬≤ = {self.meilleur_score:.4f})")
        
        return self.resultats_entrainement
    
    def evaluer_sur_test(self, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:
        """√âvalue le meilleur mod√®le sur test."""
        if self.modele_final is None:
            raise ValueError("Aucun mod√®le n'a √©t√© entra√Æn√©")
        
        # Pr√©dictions sur le test
        y_pred_test = self.modele_final.predict(X_test)
        metriques_test = self.calculer_metriques(y_test, y_pred_test)
        
        # Logger dans MLflow si possible
        try:
            with mlflow.start_run(run_name=f"test_evaluation_{self.meilleur_modele}_{self.environment}"):
                mlflow.log_param("environment", self.environment)
                mlflow.log_param("best_model", self.meilleur_modele)
                mlflow.log_param("best_run_id", self.meilleur_run_id)
                
                mlflow.log_metric("test_rmse", metriques_test['rmse'])
                mlflow.log_metric("test_mae", metriques_test['mae'])
                mlflow.log_metric("test_r2", metriques_test['r2'])
                
                # √âcart validation/test
                r2_val = self.resultats_entrainement[self.meilleur_modele]['metriques_val']['r2']
                ecart_val_test = r2_val - metriques_test['r2']
                mlflow.log_metric("generalization_gap", ecart_val_test)
                
                mlflow.set_tag("run_type", "test_evaluation")
                mlflow.set_tag("environment", self.environment)
                mlflow.set_tag("final_model", "true")
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è MLflow indisponible pour test: {str(e)}")
        
        # Calcul de l'√©cart validation/test
        r2_val = self.resultats_entrainement[self.meilleur_modele]['metriques_val']['r2']
        ecart_val_test = r2_val - metriques_test['r2']
        
        self.logger.info(f"üéØ √âvaluation test - {self.meilleur_modele}:")
        self.logger.info(f"   - R¬≤ test: {metriques_test['r2']:.4f}")
        self.logger.info(f"   - RMSE test: {metriques_test['rmse']:.2f}")
        self.logger.info(f"   - √âcart val/test: {ecart_val_test:.4f}")
        
        return metriques_test
    
    def sauvegarder_meilleur_modele(self) -> Path:
        """Sauvegarde le meilleur mod√®le selon la configuration."""
        if self.modele_final is None:
            raise ValueError("Aucun mod√®le √† sauvegarder")
        
        models_dir = self.racine_projet / "models"
        models_dir.mkdir(exist_ok=True)
        
        # Nom de fichier incluant l'environnement
        nom_fichier = f"modele_{self.meilleur_modele}_{self.environment}.pkl"
        chemin_sauvegarde = models_dir / nom_fichier
        
        # Donn√©es √† sauvegarder
        donnees_modele = {
            'modele': self.modele_final,
            'nom_modele': self.meilleur_modele,
            'environment': self.environment,
            'run_id': self.meilleur_run_id,
            'metriques_test': self.resultats_entrainement[self.meilleur_modele]['metriques_val'],
            'hyperparameters': self.modeles_config[self.meilleur_modele]['config'].hyperparameters,
            'timestamp': datetime.now().isoformat(),
            'config_version': "2.0_with_config_management"
        }
        
        joblib.dump(donnees_modele, chemin_sauvegarde)
        
        self.logger.info(f"‚úÖ Mod√®le sauvegard√©: {chemin_sauvegarde}")
        
        return chemin_sauvegarde


def main():
    """Pipeline d'entra√Ænement avec configuration centralis√©e."""
    try:
        # R√©cup√©rer l'environnement depuis les variables d'environnement ou argument
        import argparse
        parser = argparse.ArgumentParser(description="Entra√Ænement avec configuration")
        parser.add_argument('--environment', '-e', default=None,
                           help='Environnement de configuration (development, production)')
        args = parser.parse_args()
        
        print("üî• === ENTRA√éNEMENT AVEC CONFIGURATION - PHASE 2 ===")
        
        # Initialiser l'entra√Æneur avec configuration
        entraineur = EntraineurAvecConfiguration(environment=args.environment)
        
        print(f"\nüìä Configuration charg√©e:")
        print(f"   - Environnement: {entraineur.environment}")
        print(f"   - Debug: {entraineur.config.debug}")
        print(f"   - Mod√®les: {list(entraineur.modeles_config.keys())}")
        print(f"   - MLflow: {entraineur.config.mlflow.experiment_name}")
        
        print("\nüìä 1. Chargement des donn√©es...")
        X_train, X_val, X_test, y_train, y_val, y_test = entraineur.charger_donnees_preprocessees()
        
        print("\nüî• 2. Entra√Ænement avec configuration...")
        resultats = entraineur.entrainer_tous_modeles(X_train, y_train, X_val, y_val)
        
        print("\nüéØ 3. √âvaluation finale sur test...")
        metriques_test = entraineur.evaluer_sur_test(X_test, y_test)
        
        print("\nüíæ 4. Sauvegarde du meilleur mod√®le...")
        chemin_modele = entraineur.sauvegarder_meilleur_modele()
        
        print("\n‚úÖ === ENTRA√éNEMENT TERMIN√â ===")
        print(f"üèÜ Meilleur mod√®le: {entraineur.meilleur_modele}")
        print(f"üìä R¬≤ test: {metriques_test['r2']:.4f}")
        print(f"üéØ RMSE test: {metriques_test['rmse']:.2f}")
        print(f"üíæ Mod√®le sauv√©: {chemin_modele.name}")
        print(f"üî• Environnement: {entraineur.environment}")
        
        if entraineur.config.mlflow.tracking_uri:
            print(f"\nüìä Pour voir MLflow UI:")
            print(f"mlflow ui --backend-store-uri {entraineur.config.mlflow.tracking_uri} --port 5000")
        
        return entraineur
        
    except Exception as e:
        print(f"‚ùå Erreur: {str(e)}")
        return None


if __name__ == "__main__":
    entraineur = main()